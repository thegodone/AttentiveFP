{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18567114-6ab2-4d1d-a6b3-da2e953f299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1128\n",
      "number of successfully processed smiles:  1128\n",
      "not processed items\n",
      "iter per batch: 29\n",
      "RAM memory % used: 41.0 RAM Used (GB): 53.75148032\n",
      "RAM memory % used: 41.0 RAM Used (GB): 53.711044608\n",
      "RAM memory % used: 41.0 RAM Used (GB): 53.689614336\n",
      "RAM memory % used: 41.1 RAM Used (GB): 53.893316608\n",
      "RAM memory % used: 41.0 RAM Used (GB): 53.735292928\n",
      "RAM memory % used: 41.0 RAM Used (GB): 53.76065536\n",
      "RAM memory % used: 41.0 RAM Used (GB): 53.665366016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4w/xmf8nmhs51j4vjsttzcxmmm00000gn/T/ipykernel_54749/1894371239.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0mbest_test_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0mbest_test_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_test_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rmse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4w/xmf8nmhs51j4vjsttzcxmmm00000gn/T/ipykernel_54749/1894371239.py\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0mtrain_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mtest_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4w/xmf8nmhs51j4vjsttzcxmmm00000gn/T/ipykernel_54749/1894371239.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, e, doprofile)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m#print(counter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlx.core as mx\n",
    "import math\n",
    "import mlx.optimizers as optim\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlx_graphs.data import GraphData\n",
    "from mlx_graphs.datasets.dataset import Dataset\n",
    "from mlx_graphs.datasets.utils import download\n",
    "from mlx_graphs.utils.transformations import to_sparse_adjacency_matrix\n",
    "from typing import Tuple\n",
    "from typing import Optional\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Lipinski\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import mlx.optimizers as optim\n",
    "from mlx_graphs.loaders import Dataloader\n",
    "import mlx.nn as nn\n",
    "from mlx_graphs.nn import GINConv, global_mean_pool, global_max_pool, Linear\n",
    "import time\n",
    "from attfp import AttentiveFP\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from AttentiveFP import save_smiles_dicts, get_smiles_dicts, get_smiles_array\n",
    "import psutil\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "random_seed = 108 \n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "p_dropout= 0.2\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.5\n",
    "output_units_num = 1 # for regression model\n",
    "radius = 2\n",
    "T = 2\n",
    "\n",
    "task_name = 'solubility'\n",
    "tasks = ['measured log solubility in mols per litre']\n",
    "\n",
    "raw_filename = \"delaney-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)]\n",
    "# print(smiles_tasks_df)\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(feature_filename):\n",
    "    feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "else:\n",
    "    feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "# feature_dicts = get_smiles_dicts(smilesList)\n",
    "remained_df = smiles_tasks_df[smiles_tasks_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = smiles_tasks_df.drop(remained_df.index)\n",
    "print(\"not processed items\")\n",
    "uncovered_df\n",
    "\n",
    "remained_df = remained_df.reset_index(drop=True)\n",
    "test_df = remained_df.sample(frac=1/10, random_state=random_seed) # test set\n",
    "training_data = remained_df.drop(test_df.index) # training data\n",
    "\n",
    "# training data is further divided into validation set and train set\n",
    "valid_df = training_data.sample(frac=1/9, random_state=random_seed) # validation set\n",
    "train_df = training_data.drop(valid_df.index) # train set\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# print(len(test_df),sorted(test_df.cano_smiles.values))\n",
    "\n",
    "\n",
    "\n",
    "class GRUCell(nn.Module):\n",
    "    \"\"\"A GRU Cell that returns the final hidden state only.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        scale = 1.0 / math.sqrt(hidden_size)\n",
    "        self.Wx = mx.random.uniform(\n",
    "            low=-scale, high=scale, shape=(3 * hidden_size, input_size)\n",
    "        )\n",
    "        self.Wh = mx.random.uniform(\n",
    "            low=-scale, high=scale, shape=(3 * hidden_size, hidden_size)\n",
    "        )\n",
    "        self.b = (\n",
    "            mx.random.uniform(low=-scale, high=scale, shape=(3 * hidden_size,))\n",
    "            if bias\n",
    "            else None\n",
    "        )\n",
    "        self.bhn = (\n",
    "            mx.random.uniform(low=-scale, high=scale, shape=(hidden_size,))\n",
    "            if bias\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, hidden=None):\n",
    "        if self.b is not None:\n",
    "            x = mx.addmm(self.b, x, self.Wx.T)\n",
    "        else:\n",
    "            x = x @ self.Wx.T\n",
    "\n",
    "        x_rz = x[..., : -self.hidden_size]\n",
    "        x_n = x[..., -self.hidden_size :]\n",
    "\n",
    "        for idx in range(x.shape[-2]):\n",
    "            rz = x_rz[..., idx, :]\n",
    "            if hidden is not None:\n",
    "                h_proj = hidden @ self.Wh.T\n",
    "                h_proj_rz = h_proj[..., : -self.hidden_size]\n",
    "                h_proj_n = h_proj[..., -self.hidden_size :]\n",
    "\n",
    "                if self.bhn is not None:\n",
    "                    h_proj_n += self.bhn\n",
    "\n",
    "                rz = rz + h_proj_rz\n",
    "\n",
    "            rz = mx.sigmoid(rz)\n",
    "\n",
    "            r, z = mx.split(rz, 2, axis=-1)\n",
    "\n",
    "            n = x_n[..., idx, :]\n",
    "\n",
    "            if hidden is not None:\n",
    "                n = n + r * h_proj_n\n",
    "            n = mx.tanh(n)\n",
    "\n",
    "            if hidden is not None:\n",
    "                hidden = (1 - z) * n + z * hidden\n",
    "            else:\n",
    "                hidden = (1 - z) * n\n",
    "\n",
    "        return hidden\n",
    "\n",
    "class AttFP(nn.Module):\n",
    "    def __init__(self, radius, T, input_feature_dim, input_bond_dim, fingerprint_dim, output_units_num, p_dropout=0.1):\n",
    "        super(AttFP, self).__init__()\n",
    "        \n",
    "        self.atom_fc =  nn.Linear(input_feature_dim, fingerprint_dim)\n",
    "        self.neighbor_fc =  nn.Linear(input_feature_dim + input_bond_dim, fingerprint_dim)\n",
    "        self.GRUCell = [GRUCell(fingerprint_dim, fingerprint_dim) for r in range(radius)]\n",
    "        self.align = [nn.Linear(2 * fingerprint_dim, 1) for r in range(radius)]\n",
    "        self.attend = [nn.Linear(fingerprint_dim, fingerprint_dim) for r in range(radius)]\n",
    "        \n",
    "        self.molGRU =  GRUCell(fingerprint_dim, fingerprint_dim)\n",
    "        self.mol_align = nn.Linear(2 * fingerprint_dim, 1)\n",
    "        self.mol_attend = nn.Linear(fingerprint_dim, fingerprint_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "        self.output = nn.Linear(fingerprint_dim, output_units_num)\n",
    "\n",
    "        self.radius = radius\n",
    "        self.T = T\n",
    "\n",
    "\n",
    "    def __call__(self, atom_list, bond_list, atom_degree_list, bond_degree_list, atom_mask):\n",
    "  \n",
    "        atom_mask = atom_mask[:,:,None]\n",
    "        batch_size, mol_length, num_atom_feat = atom_list.shape\n",
    "        \n",
    "        atom_feature = nn.leaky_relu(self.atom_fc(mx.array(atom_list)))\n",
    "        \n",
    "        bond_neighbor = [mx.array(bond_list[i][bond_degree_list[i]]) for i in range(batch_size)]\n",
    "        bond_neighbor = mx.stack(bond_neighbor, axis=0)\n",
    "                \n",
    "        atom_neighbor = [mx.array(atom_list[i][atom_degree_list[i]]) for i in range(batch_size)]\n",
    "        atom_neighbor = mx.stack(atom_neighbor, axis=0)\n",
    "                \n",
    "        neighbor_feature = mx.concatenate([atom_neighbor, bond_neighbor], axis=-1)\n",
    "        neighbor_feature = nn.leaky_relu(self.neighbor_fc(neighbor_feature))\n",
    "                \n",
    "        # Generate mask to eliminate the influence of blank atoms\n",
    "        attend_mask = mx.array(atom_degree_list)\n",
    "        attend_mask = mx.where(attend_mask == mol_length - 1, mx.array(0.0), mx.array(1.0))\n",
    "        attend_mask = attend_mask[:,:,:,None]\n",
    "        \n",
    "        softmax_mask = mx.array(atom_degree_list)\n",
    "        softmax_mask = mx.where(softmax_mask == mol_length - 1,  mx.array(float(\"-inf\")),mx.array(0.0))\n",
    "        softmax_mask =  mx.expand_dims(softmax_mask, 3)\n",
    "        \n",
    "        batch_size, mol_length, max_neighbor_num, fingerprint_dim = neighbor_feature.shape\n",
    "                \n",
    "        atom_feature_expand = mx.expand_dims(atom_feature, 2) #[:,:,None,:]\n",
    "        atom_feature_expand = mx.repeat(atom_feature_expand,  max_neighbor_num, axis=2)\n",
    "        feature_align = mx.concatenate([atom_feature_expand, neighbor_feature], axis=-1)\n",
    "        \n",
    "        align_score = nn.leaky_relu(self.align[0](self.dropout(feature_align)))\n",
    "        align_score = align_score + softmax_mask  # Ensure both tensors are on the same device\n",
    "        attention_weight = nn.softmax(align_score, -2)\n",
    "        attention_weight = attention_weight * attend_mask\n",
    "        \n",
    "        neighbor_feature_transform = self.attend[0](self.dropout(neighbor_feature))\n",
    "        context = mx.sum(attention_weight * neighbor_feature_transform, axis=-2)\n",
    "        context = nn.elu(context)\n",
    "        \n",
    "        context_reshape = mx.reshape(context,(batch_size * mol_length, fingerprint_dim))\n",
    "        atom_feature_reshape = mx.reshape(atom_feature,(batch_size * mol_length, fingerprint_dim))\n",
    "        \n",
    "        atom_feature_reshape = self.GRUCell[0](context_reshape, atom_feature_reshape)\n",
    "        atom_feature = mx.reshape(atom_feature_reshape,(batch_size, mol_length, fingerprint_dim))\n",
    "        \n",
    "        activated_features = nn.relu(atom_feature)\n",
    "        \n",
    "        for d in range(1,self.radius):\n",
    "        \n",
    "            atom_degree_list_mx = mx.array(atom_degree_list)\n",
    "            neighbor_feature = [activated_features[i][atom_degree_list_mx[i]] for i in range(batch_size)]\n",
    "            neighbor_feature = mx.stack(neighbor_feature, axis=0)\n",
    "        \n",
    "            atom_feature_expand = mx.expand_dims(activated_features, 2) #[:,:,None,:]\n",
    "            atom_feature_expand = mx.repeat(atom_feature_expand,  max_neighbor_num, axis=2)\n",
    "            feature_align = mx.concatenate([atom_feature_expand, neighbor_feature], axis=-1)\n",
    "            \n",
    "            align_score = nn.leaky_relu(self.align[d](self.dropout(feature_align)))\n",
    "            align_score = align_score + softmax_mask  # Ensure both tensors are on the same device\n",
    "            attention_weight = nn.softmax(align_score, -2)\n",
    "            attention_weight = attention_weight * attend_mask\n",
    "            \n",
    "            neighbor_feature_transform = self.attend[d](self.dropout(neighbor_feature))\n",
    "            context = mx.sum(attention_weight * neighbor_feature_transform, axis=-2)\n",
    "            context = nn.elu(context)\n",
    "            context_reshape = mx.reshape(context,(batch_size * mol_length, fingerprint_dim))\n",
    "            \n",
    "            atom_feature_reshape = self.GRUCell[d](context_reshape, atom_feature_reshape)\n",
    "            atom_feature = mx.reshape(atom_feature_reshape,(batch_size, mol_length, fingerprint_dim))\n",
    "            activated_features = nn.relu(atom_feature)\n",
    "        \n",
    "        mol_feature = mx.sum(activated_features * atom_mask, axis=-2)\n",
    "        \n",
    "        activated_features_mol = nn.relu(mol_feature)\n",
    "        \n",
    "        mol_softmax_mask = mx.array(atom_mask)\n",
    "        mol_softmax_mask = mx.where(mol_softmax_mask == 1,  mx.array(0.0), mx.array(float(\"-inf\")))  \n",
    "        # this one is strange \n",
    "        #mol_softmax_mask[mol_softmax_mask == 0] = -9e8\n",
    "        #mol_softmax_mask[mol_softmax_mask == 1] = 0\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            mol_prediction_expand = mx.expand_dims(activated_features_mol, 1) \n",
    "            mol_prediction_expand = mx.repeat(mol_prediction_expand,  mol_length, axis=1)\n",
    "            \n",
    "            mol_align = mx.concatenate([mol_prediction_expand, activated_features], axis=-1)\n",
    "        \n",
    "             \n",
    "            mol_align_score = nn.leaky_relu(self.mol_align(mol_align))\n",
    "            mol_align_score = mol_align_score + mol_softmax_mask  # Ensure both tensors are on the same device\n",
    "            mol_attention_weight = nn.softmax(mol_align_score, -2)\n",
    "            mol_attention_weight = mol_attention_weight * atom_mask\n",
    "            \n",
    "            activated_feature_transform = self.mol_attend(self.dropout(activated_features))\n",
    "            mol_context = mx.sum(mol_attention_weight * activated_feature_transform, axis=-2)\n",
    "            mol_context = nn.elu(mol_context)    \n",
    "            mol_feature = self.molGRU(mol_context, mol_feature)\n",
    "            activated_features_mol = nn.relu(mol_feature)\n",
    "        \n",
    "        mol_prediction = self.output(self.dropout(mol_feature))\n",
    "\n",
    "        return atom_feature, mol_prediction\n",
    "\n",
    "\n",
    "device = mx.gpu # or mx.cpu\n",
    "mx.set_default_device(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([canonical_smiles_list[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "model = AttFP(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "\n",
    "def loss_fn(y_hat, y, parameters=None):\n",
    "    if len(y_hat.shape) != len(y.shape):\n",
    "        y = mx.expand_dims(y,1)\n",
    "    return mx.mean(nn.losses.mse_loss(y_hat, y))\n",
    "\n",
    "def forward_fn(model, x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, labels):\n",
    "    _, y_hat = model(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask)\n",
    "    loss = loss_fn(y_hat, labels, model.parameters())\n",
    "    return loss, y_hat\n",
    "        \n",
    "def train(dataset,e, doprofile=False):\n",
    "\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    np.random.seed(e)\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    print('iter per batch:',len(batch_list))\n",
    "    if doprofile:\n",
    "\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()  # Start profiling\n",
    "    for counter, train_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[train_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        y_val = mx.array(batch_df[tasks[0]].values)\n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        \n",
    "        (loss, y_hat), grads = loss_and_grad_fn(\n",
    "            model=model,\n",
    "            x_atom=x_atom,\n",
    "            x_bonds=x_bonds,\n",
    "            x_atom_index=x_atom_index,\n",
    "            x_bond_index=x_bond_index,\n",
    "            x_mask=x_mask,\n",
    "            labels=y_val,\n",
    "        )\n",
    "        \n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        loss_sum += loss.item()\n",
    "        #print(counter)\n",
    "        print('RAM memory % used:', psutil.virtual_memory()[2],'RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "    if doprofile:\n",
    "        pr.disable()\n",
    "        # Print profiling results\n",
    "        s = io.StringIO()\n",
    "        ps = pstats.Stats(pr, stream=s).sort_stats(pstats.SortKey.CUMULATIVE)\n",
    "        ps.print_stats(200)  # Print top 10 results\n",
    "        print(s.getvalue())    \n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2],'RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "    return loss_sum / len(dataset)\n",
    "\n",
    "\n",
    "\n",
    "def test(test_dataset):\n",
    "    mse= 0.0\n",
    "    valList = np.arange(0,test_dataset.shape[0])\n",
    "    batch_list = []\n",
    "    for i in range(0, test_dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "        \n",
    "    for counter, test_batch in enumerate(batch_list):\n",
    "        batch_df = test_dataset.loc[test_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        y_val = mx.array(batch_df[tasks[0]].values)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, _ = get_smiles_array(smiles_list,feature_dicts)\n",
    "        _, y_hat = model(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask)\n",
    "       \n",
    "        mse += mx.square(y_hat - y_val).sum().item()\n",
    "        \n",
    "    val =  mse / len(test_dataset)\n",
    "    return val, np.sqrt(val)\n",
    "\n",
    "\n",
    "def epoch(e):\n",
    "    loss = train(train_df,e)\n",
    "    train_mse, train_rmse = test(train_df)\n",
    "    test_mse, test_rmse = test(test_df)\n",
    "    return loss, train_mse, train_rmse, test_mse, test_rmse\n",
    "\n",
    "\n",
    "\n",
    "mx.eval(model.parameters())\n",
    "optimizer = optim.AdamW(10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, forward_fn)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "r = []\n",
    "best_test_mse = 1e9\n",
    "for e in range(epochs):\n",
    "    loss, train_mse, train_rmse, test_mse, test_rmse = epoch(e)\n",
    "    best_test_mse = min(best_test_mse, test_mse)\n",
    "    r.append((train_rmse,test_rmse))\n",
    "    print(\n",
    "        \" | \".join(\n",
    "            [\n",
    "                f\"Epoch: {e:3d}\",\n",
    "                f\"Train loss: {loss:.3f}\",\n",
    "                f\"Train mse: {train_mse:.3f}\",\n",
    "                f\"Train rmse: {train_rmse:.3f}\",\n",
    "                f\"Test mse: {test_mse:.3f}\",\n",
    "                f\"Test rmse: {test_rmse:.3f}\",\n",
    "                f\"LR: {np.array(optimizer.learning_rate)}\",\n",
    "\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "print(f\"\\n==> Best test mse: {best_test_mse:.3f},  rmse: {np.sqrt(best_test_mse):.3f}\")\n",
    "\n",
    "\n",
    "#train(train_df,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977330c-78fd-4737-85fd-d1d65c78ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train1(dataset,e):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    np.random.seed(e)\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    print(len(batch_list))\n",
    "    train_batch =       batch_list[0]\n",
    "\n",
    "    batch_df = dataset.loc[train_batch,:]\n",
    "    smiles_list = batch_df.cano_smiles.values\n",
    "    y_val = mx.array(batch_df[tasks[0]].values)\n",
    "    x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "    # Profiler setup\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()  # Start profiling\n",
    "\n",
    "    (loss, y_hat), grads = loss_and_grad_fn(\n",
    "        model=model,\n",
    "        x_atom=x_atom,\n",
    "        x_bonds=x_bonds,\n",
    "        x_atom_index=x_atom_index,\n",
    "        x_bond_index=x_bond_index,\n",
    "        x_mask=x_mask,\n",
    "        labels=y_val,\n",
    "    )\n",
    "    \n",
    "    optimizer.update(model, grads)\n",
    "    mx.eval(model.parameters(), optimizer.state)\n",
    "    # Stop profiling\n",
    "    pr.disable()\n",
    "\n",
    "    # Print profiling results\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats(pstats.SortKey.CUMULATIVE)\n",
    "    ps.print_stats(200)  # Print top 10 results\n",
    "    print(s.getvalue())    \n",
    "    return loss_sum / len(dataset)\n",
    "\n",
    "\n",
    "def epoch1(e):\n",
    "    loss = train1(train_df,e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
